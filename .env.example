# ─────────────────────────────────────────────────────────────────────────────
# PYTHIA — environment variables
# Copy this file to .env and fill in your values.
# ─────────────────────────────────────────────────────────────────────────────

# Ollama / OpenAI-compatible LLM endpoint
# Can be on the same host (Jetson) or any other machine on the network.
# The backend calls: POST {OLLAMA_URL}/v1/chat/completions
# Si Ollama tourne nativement sur le même host que Docker (recommandé sur Jetson) :
OLLAMA_URL=http://host.docker.internal:11434
# Si Ollama tourne sur une autre machine du réseau :
# OLLAMA_URL=http://192.168.1.100:11434
OLLAMA_MODEL=Jadio/Qwen3_4b_instruct_q4km

# Language for LLM system prompt and UI labels ("en" or "fr")
PYTHIA_LANGUAGE=en

# Events with criticality below this threshold are silently discarded
CRITICALITY_THRESHOLD=1

# Backend port (exposed by Docker)
BACKEND_PORT=8000

# Frontend port (exposed by Docker)
FRONTEND_PORT=80

# Path to sources configuration (relative to backend working dir inside Docker)
SOURCES_FILE=sources.yaml
